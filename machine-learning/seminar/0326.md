#   Machine Learning

back propagation: 오차에 따라 가중치를 조절하는 과정


##  예시
chain rule

$$
f \left( x, y, z \right) = (x+y)z


\\
\textrm{forward propagation}
q =  x+ y
\\
f = qz

\\

x=-2, y=5, z=-4
\\
f = -12

\\
$$

왜 굳이 편미분을 하느냐?

뒤에서부터 개선하면 머리가 아프니 미분값(local gradient)을 저장해놓는다.

마지막에 필요한 미분값을 global gradient라고 한다.

상수뿐만 아니라 변수도 저장할 수 있다.

chain rule: 미리 저장한 걸 둘이 곱한다.

$$
{{\partial f} \over {\partial z}} = {{\partial f} \over { \partial x}} \cdot {{\partial f} \over { \partial y}}
$$

### 실습
*   `nn.Sequential`: distribution parralel에 들어맞는 api. 그러니까 얘를 주로 쓰자.

*   `def forward`만 잘 정의하면 backward를 신경쓸 필요가 없다.
    *   **`forward`를 먼저 보라.**

*   `dropout`: 랜덤하게 노드를 죽이기(인자가 0.25면 노드를 1/4 확률로 죽인다는 뜻)
    *   정규화 효과가 있다고 함.
    *   validation 또는 prediction 중에는 죽이지 않음.
    *   `mode.train()` 시에는 dropout activation됨.
    *   `mode.eval()` 시에는 dropout이 activation되지 않음.

*   view와 flatten의 성능 차이는 별로 없음.
    *   `.view(-1)`는 `.flatten(x, 1)`와 같음.

CIFAR Framework

*   pytorch lightening
    *   `pl.LightningModule`

*   model이라는 거는 각 component의 float value(item, in pytorch word)인데 이걸 dictionary(`model.state_dict()`)로 저장.

####    사이즈
B, C, W, H
*   Batch size
*   Channel (또는 Dimension)
*   Width
*   Height

### cross entropy
*   true positive: 맞는 걸 맞음.
*   true negative: 틀린 걸 틀리다고 함.
*   false positive: 아닐 걸 맞다고 하기.
*   false negative: 맞는 걸 아니라고 하기.

*   classification class의 이슈: n차원을 평면으로 그었을 떄 얼마나 잘 나눌 수 있을까.

*   latent space: ??


*   미분불가능한 함수를 쓰면 latent space가 각지게 접힌다.
*   ELU는 미분가능해서 latent space가 매끄럽게 접힌다.

*   residual connection: 이전 인자 값을 다시 더해서 gradient explosion이나 vanishment를 막는데 도움이 됨.


##  validation
validation data set은 따로 한다.
대신 forward propagation (`torch.no_grad()`, gradient 업데이트 없음)만 한다.

validation loader에서 성능 병목이 발생할 수 있다.


##  Multi-Label
`loss = f.multilabel_soft_margin_loss(x, labels)`

##  Optimizer
요즘 트렌드는 AdamW

*   Trainer oaraneter
    *   `precision='mixed'`: forward propagation 시에는 16, backward propagation 시에는 32
