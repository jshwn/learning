#   캐시 메모리
*   공간 지역성: 최근에 참조한 데이터 근처를 곧바로 다시 참조한다.
*   시간 지역성: 최근에 참조한 데이터를 곧바로 다시 참조한다.

데이터가 캐시 내에 있는지 어떻게 알 수 있는가?
데이터가 캐시 내에 있다면, 어떻게 찾을 수 있는가?


##  직접 사상 방식
주소 크기가 n비트이고, 캐시 내 블록의 수가 c인 경우
(n과 c 모두 2의 거듭제곱수이다.)

예: n=8비트, c=8인 경우,
캐시 블록의 수가 8이므로, 주소의 하위 3비트를 인덱스로 사용($\log_2 8=3$)하고 나머지 상위 5비트는 태그 비트로 사용한다.
태그 비트를 사용할 때 상위 5비트만 사용하는 이유는 캐시 인덱스를 참조하는 것 자체가 이미 하위 3비트는 맞는 주소 비트들이기 때문이다.
(MIPS에서는 바이트 오프셋 때문에 하위 2비트를 아예 배제한다(그림 5.10).)

유효 비트는 초기값은 0이고, 한 번이라도 참조되었다면 1로 바뀐다.
컴퓨터 실행 초기에는 캐시 메모리에 쓰레기 값(또는 0)이 들어있는데, 이를 참조하여 발생하는 버그를 원천 차단하기 위함이다.

*   캐시 인덱스
*   태그 비트
*   유효 비트: 초기값은 0이고, 한 번이라도 참조되었다면 1로 바뀐다.
*   실제 데이터


블록의 크기가 1바이트면 모든 주소값이 캐시 인덱스에 매핑된다.
캐시 메모리의 크기에 따라 다를 수 있지만 일반적으로 캐시 메모리가 메인 메모리보다 크기가 훨씬 작은 것을 감안하면, 하나의 캐시 인덱스에 엄청나게 많은 주소값이 매핑되므로 캐시 미스가 자주 발생하여 실패율이 높아질 것이다.

따라서 일반적으로 블록의 크면 공간적 지역성으로 실패율이 낮아진다.
하지만 캐시 크기가 한정되어 있는데 블록 크기만 커지면 캐시 인덱스는 적어질 수밖에 없다.
즉, 캐시 블록 개수 자체가 줄어들게 된다.
그런데 캐시 내 블록 개수가 너무 부족해지면, 블록에 대한 경쟁이 심해진다.

하지만 블록 크기가 증가는 실패율보다 실패비용이 훨씬 중요하다.
즉 블록 크기가 클수록 캐시 미스가 발생했을 떄, 다음 계층에서 블록을 가져오는 (시간적) 비용이 커진다.
이 실패 비용에는 첫 번째 워드를 가져오는 데 걸리는 접근 지연(latency)과 블록의 나머지 부분에 대한 전송 시간이 포함된다.

*   조기 재시작: 블록 내 워드가 도달하면 나머지 워드가 도착하는 것을 기다리지 않고 해당 워드에 대한 작업을 바로 실행
*   요구 워드 우선(또는 중요 워드 우선): 블록 내 첫 워드가 아니라 요청된 워드 순서대로 캐시로 전송시키는 것.

##  쓰기의 처리
*   즉시 쓰기(write-through)
*   쓰기 버퍼(write buffer)
*   나중 쓰기(write-back)

즉시 쓰기는 전체 프로그램에서 메모리 적재 명령어가 많으면 매우 비효율적이게 된다.
왜냐하면 캐시에도 쓰고 메모리에 값을 쓸 때까지 프로세서가 기다려야 하기 때문이다.

쓰기 버퍼(write buffer)의 경우, 일단 데이터를 캐시에 쓰고, 쓰기 버퍼에 넣는다.
그리고 쓰기 버퍼는 순서대로 메모리애 데이터를 쓰고, 메모리에 쓰인 데이터는 버퍼에서 지워진다.
그런데 쓰기 버퍼가 하나의 쓰기를 처리하는 시간보다 쓰기가 발생하는 시간이 빠르면 지연이 발생한다.

나중 쓰기는 데이터를 캐시에만 썼다가, 데이터가 캐시에서 쫓겨날 때 메모리에 해당 데이터를 쓰는 방식이다.
물론 나중 쓰기 방식이 즉시 쓰기 방식보다 구현이 어렵다.

##  쓰기 실패에 대한 처리
즉시 쓰기에 대한 실패: 쓰기 할당(write allocate), 쓰기 비하랑(no write allocate)


##  집합 사상 방식, 완전 사상 방식
연관 정도를 늘리면 실패율이 줄어든다는 장점이 있지만, 적중 시간이 증가한다는 단점이 있다.

직접 사상 방식은 1-way 집합 연관 캐시이다.
블록이 n개일 때, n-way 집합 연관 캐시는 완전 연관 캐시라고 한다.

참고로 캐시의 크기와 연관 정도는 독립적으로 캐시의 성능(캐시 성공율)을 결정하지 못한다.

CAM: Content Addressable Memory


##  다단계 캐시를 이용해 실패 손실 줄이기

단일 계층 캐시와 다단계 캐시에서는 최적은 선택이 달라진다.
예를 들어 1차 캐시에서는 클럭 사이클이나 파이프라인 단계 축소가 가능하도록 시간 최소화에 초점을 두고, 2차 캐시에서는 접근시간으로 인한 손실을 줄이도록 실패율에 중점을 둔다.

##  블로킹을 이용한 소프트웨어 최적화
배열을 행 우선 순서로 저장하냐 열 우선 순서로 저장하냐는 행 또는 열 우선 접근을 모두 사용해야 할 때는 성능 개선에 큰 도움이 되지 않는다.

블록화 알고리즘은 배열의 행과 열 전체를 처리하는 대신 부분행렬(submatrix) 또는 블록 단위로 처리한다.
캐시에 적재된 데이터가 교체되기 전에 최대한 사용하는 것이 목표이다.
즉, 캐시 실패를 줄이기 위해 시간적 지역성을 향상시키는 것이다.


### 다단계 캐시에서 고려하는 추가적인 실패율 지표
*   전역 실패율(global cache miss): 모든 계층에서 참조에 실패한 비율
*   지역 실패율(local cache miss): 해당 단계의 캐시 참조 실패 횟수 / 해당 단계의 캐시 참조(또는 접근) 횟수

##  신용도 있는 메모리 계층 구조(생략)
Hamming 단일 에러 정정, 이중 에러 검출 코드(Hamming ECC)

IBM의 chipkill, Intel의 SDDC

CRC(Cyclic Redundancy Check), Reed-Solomon 코드(갈루아 체 Galois Field를 사용)


##  p492
많은 시스템들은 주소 공간을 고정 크기의 큰 블록들로 나누는데, 이렇게 하면 운영체제와 사용자 프로그램 사이의 보호 기능이 단순화되고 페이징 구현의 효율성이 증대된다. 이런 블록을 "세그먼트"라고 부르는 경우도 있지만, 이 방식은 가변 크기를 갖는 세그멘테이션 방식보다 훨씬 간단하고 사용자 프로그램에는 보이지 않는다.